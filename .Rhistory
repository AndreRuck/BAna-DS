<<<<<<< Updated upstream
)
Sys.time()
View(features_train_bt)
View(m.bt)
m.bt[["evaluation_log"]]
#number of trees that minimize error
m.bt$evaluation_log %>%
dplyr::summarise(
ntrees.train = which(train_rmse_mean == min(train_rmse_mean))[1],
rmse.train   = min(train_rmse_mean),
ntrees.test  = which(test_rmse_mean == min(test_rmse_mean))[1],
rmse.test   = min(test_rmse_mean),
)
#number of trees that minimize error
m.bt$evaluation_log %>%
dplyr::summarise(
ntrees.train = which(train_rmse_mean == min(train_rmse_mean))[1],
rmse.train   = min(train_rmse_mean),
ntrees.test  = which(test_rmse_mean == min(test_rmse_mean))[1],
rmse.test   = min(test_rmse_mean),
)
# plot error vs number trees
ggplot(xgb.fit1$evaluation_log) +
geom_line(aes(iter, train_rmse_mean), color = "red") +
geom_line(aes(iter, test_rmse_mean), color = "blue")
# plot error vs number trees
ggplot(m.bt$evaluation_log) +
geom_line(aes(iter, train_rmse_mean), color = "red") +
geom_line(aes(iter, test_rmse_mean), color = "blue")
#Aci model
featuresBt_aci <- as.matrix(select(ohieVariables_nNA, !c("any_ed_psychiatric_condition_or_substance_abuse", "food_assistance",
"temporary_assistance", "charge_total")))
#Initial model
featuresBt <- as.matrix(select(ohieVariables_nNA, !c("charge_total")))
m.bt <- xgb.cv(
data = featuresBt,
label = ohieVariables_nNA$charge_total,
nrounds = 1400,
nfold = 5,    #one is for testing the rest is used for training
objective = "reg:squarederror",  # for regression models, linear is depreciated
verbose = 0   # silent,
)
View(m.bt)
m.bt[["evaluation_log"]]
#number of trees that minimize error
m.bt$evaluation_log %>%
dplyr::summarise(
ntrees.train = which(train_rmse_mean == min(train_rmse_mean))[1],
rmse.train   = min(train_rmse_mean),
ntrees.test  = which(test_rmse_mean == min(test_rmse_mean))[1],
rmse.test   = min(test_rmse_mean),
)
# plot error vs number trees
ggplot(m.bt$evaluation_log) +
geom_line(aes(iter, train_rmse_mean), color = "red") +
geom_line(aes(iter, test_rmse_mean), color = "blue")
?xgb.cv
for (i in 1:7)
{
1+1
}
1+1
?list
?vector
depthList_m.bt_aci <- vector(mode = "list", length = 8)
for (i in 1:8)
{
set.seed(1111)
depthList_m.bt_aci[i] <- 2*i
}
View(depthList_m.bt_aci)
i=1
depthList_m.bt_aci[i] <- xgb.cv(
data = featuresBt_aci,
label = ohieVariables_nNA$charge_total,
nrounds = 800,
nfold = 5,    #one is for testing the rest is used for training
objective = "reg:squarederror",  # for regression models, linear is depreciated
verbose = 0,   # silent,
max_depth = i)
View(depthList_m.bt_aci)
depthList_m.bt_aci[[1]]
depthList_m.bt_aci[i] <- xgb.cv(
data = featuresBt_aci,
label = ohieVariables_nNA$charge_total,
nrounds = 800,
nfold = 5,    #one is for testing the rest is used for training
objective = "reg:squarederror",  # for regression models, linear is depreciated
verbose = 0,   # silent,
)
?xgb.cv
View(depthList_m.bt_aci)
depthList_m.bt_aci <- vector(mode = "list", length = 8)
depthList_m.bt_aci[1] <- xgb.cv(
data = featuresBt_aci,
label = ohieVariables_nNA$charge_total,
nrounds = 800,
nfold = 5,    #one is for testing the rest is used for training
objective = "reg:squarederror",  # for regression models, linear is depreciated
verbose = 0)   # silent,
depthList_m.bt_aci[2] <- lm(charge_total ~ ., train_nNA, na.action = "na.fail")
View(depthList_m.bt_aci)
depthList_m.bt_aci[[2]] <- lm(charge_total ~ ., train_nNA, na.action = "na.fail")
View(depthList_m.bt_aci)
depthList_m.bt_aci[[1]] <- xgb.cv(
data = featuresBt_aci,
label = ohieVariables_nNA$charge_total,
nrounds = 800,
nfold = 5,    #one is for testing the rest is used for training
objective = "reg:squarederror",  # for regression models, linear is depreciated
verbose = 0)   # silent,
1|0
View(ohie)
install.packages(c("factoextra", "ggpubr", "NbClust"))
library(ggplot2)
library(tidyverse)
library(hrbrthemes)
library(viridis)
library(ggridges)
library(corrplot)
library(ggpubr)
library(moments)
library(Hmisc)
library(factoextra)
library(NbClust)
library(car)
library(stargazer)
options(scipen=999)
##### Part 1 #############################################################################################
# Import and inspection
insurance <- read_csv("insurance.csv")
insurance
str(insurance)
summary(insurance)
##### Part 2 ###############################################################################################
## Transformation: Factor and log for positive skewness expenses
insurance$sex <- factor(insurance$sex)
insurance$smoker <- factor(insurance$smoker)
insurance$region <- factor(insurance$region)
insurance$children <- as.factor(insurance$children)
insurance$logexpenses = log(insurance$expenses)
##### Part 3 ###############################################################################################
### Visualize Distribution ####
# Histogram 1: expenses. Positive skew, as a result transformation needed for regression analysis.
hist(insurance$expenses, xlab="Expenses", ylab="Count", main="Distribution of Expenses", breaks=200)
#Density 1.1: expenses. Other example with density.
ggplot(insurance, aes(x=expenses)) +
geom_density(color="darkblue", fill="lightblue") + labs(title="expenses")
#Histogram log expenses 2: More like normal distribution
hist(insurance$logexpenses, xlab=" log Expenses", ylab="Count", main="Distribution of log Expenses", breaks=100)
#Density 2.1: logexpenses. Other example with density.
ggplot(insurance, aes(x=logexpenses)) +
geom_density(color="darkblue", fill="brown") + labs(title="expenses")
# Density 3: expenses per smoker: much riskcosts for smokers
ggplot(insurance, aes(x=expenses, fill=smoker)) +
geom_density(alpha=0.4) + theme(legend.position="bottom") + labs(title="expenses / smoker")
# Density 4: bmi & children: less bmi with 5 children
ggplot(insurance, aes(x = bmi, y = children, fill = children)) +
geom_density_ridges() + theme_ridges() + ylab("children")+ xlab("bmi") + scale_y_discrete(limits=c("0","1", "2", "3", "4", "5"))
#Boxplot 5: region & expenses#
insurance %>%
ggplot( aes(x=region, y=expenses, fill=region)) +
geom_boxplot() +
scale_fill_viridis(discrete = TRUE, alpha=0.6) +
geom_jitter(color="black", size=0.4, alpha=0.9) +
theme_ipsum() +
theme(
legend.position="none",
plot.title = element_text(size=11)
) +
ggtitle("expenses per region") +
xlab("region") + ylab("region")
#Violin 6: children & expenses
insurance$children = with(insurance, reorder(children, expenses, median))
insurance %>%
ggplot( aes(x=children, y=expenses, fill=children)) +
geom_violin() +
xlab("children") + scale_x_discrete(limits=c("0","1", "2", "3", "4", "5"))
theme(legend.position="none") +
ylab("expenses")
#Scatter 7: bmi & expenses with smoking: Smokers pay much more
ggplot(insurance, aes(x=bmi, y=expenses)) + geom_point(aes(color=smoker))
#Scatter 7.1: There are 3 important clusters
ggplot(insurance, aes(x=bmi, y=expenses)) +
geom_point(aes(color=age)) + scale_color_gradientn(colors = c("#00AFBB", "#E7B800", "#FC4E07"))
#Scatter 7.2: Nice regression by age with different stages
ggplot(insurance, aes(x=age, y=expenses)) +
geom_point(aes(color=bmi)) +  scale_color_gradient(low = "yellow", high = "darkblue")  + facet_grid(.~smoker)
##### Part 4 ###############################################################################################
#### Correlation
# Tidy
insurance.cor <- read_csv("insurance.csv")
insurance$children <- as.numeric(insurance$children)
insurance.cor$sex <- as.numeric(ifelse(insurance.cor$sex == "female",0,1))
insurance.cor$smoker <- as.numeric(ifelse(insurance.cor$smoker == "yes",1,0))
insurance_cor_tidy <- insurance.cor[,-6]
# Corrplot: smoking and age strong negative and moderat positve corelation with expenses
insurance_cor_plot <- round(cor(insurance_cor_tidy),2)
corrplot(insurance_cor_plot , method="color")
corrplot(insurance_cor_plot , method="number")
# Test both correlations: p-value
cor.test(insurance_cor_tidy$smoker, insurance_cor_tidy$expenses, method = c("pearson"))
cor.test(insurance_cor_tidy$age, insurance_cor_tidy$expenses, method = c("pearson"))
# Statistics nice displayed
mod.corr <- rcorr(as.matrix(insurance_cor_tidy), type = "pearson")
stargazer(data.frame(mod.corr$r), summary=F, type="text")
stargazer(data.frame(mod.corr$P), summary=F, type="html", out ="cor.html")
stargazer(data.frame(mod.corr$P), summary=F, type="text")
##### Part 5 ###############################################################################################
### Aggregate statistics
insurance.num <- insurance_cor_tidy
insurance.agg <- select_if(insurance.num, is.numeric)
## Function
stat.summary <- function(insurance.agg) {
total <- data.frame(
n  = sapply(insurance.agg, length),
mean = sapply(insurance.agg, mean, na.rm=T),
median = sapply(insurance.agg, median, na.rm=T),
sd = sapply(insurance.agg, sd, na.rm=T),
var= sapply(insurance.agg, var, na.rm=T),
kurt= sapply(insurance.agg, kurtosis, na.rm=T),
skew= sapply(insurance.agg, skewness, na.rm=T),
q1 = sapply(insurance.agg, quantile, probs = 0.25, na.rm=T),
q3 = sapply(insurance.agg, quantile, probs = 0.75, na.rm=T),
min =  sapply(insurance.agg, min, na.rm=T),
max =  sapply(insurance.agg, max, na.rm=T)
)
return(total)
}
# Displayed
stat.summary <- round(stat.summary(insurance.agg),2)
stargazer(stat.summary, type="text", summary=F)
stargazer(stat.summary, type="html", summary=F, out ="overview.html")
##### Part 5 ###############################################################################################
#### Multi-Regression model with different features ###
insurance <- read_csv("insurance.csv")
insurance$sex <- factor(insurance$sex)
insurance$smoker <- factor(insurance$smoker)
insurance$region <- factor(insurance$region)
insurance$children <- factor(insurance$children)
insurance$logexpenses = log(insurance$expenses)
# preparing diffrent features
FORMULA <- expenses ~ age + sex + bmi + children + smoker + region
ols.lm <- lm(data=insurance, FORMULA)
summary(ols.lm)
mod.ols1 <- update(ols.lm,   ~  smoker)
mod.ols2 <- update(ols.lm, .  ~  smoker + bmi )
mod.ols3 <- update(ols.lm, .  ~  smoker + bmi + region)
mod.ols4 <- update(ols.lm, .  ~  smoker + bmi + region + sex)
mod.ols5 <- update(ols.lm, .  ~  smoker + bmi + region + sex + children )
mod.ols6 <- update(ols.lm, .  ~  smoker + bmi + region + sex + children + age )
mod.ols7 <- update(ols.lm, .  ~  smoker + age + sex + bmi)
stargazer(mod.ols1, mod.ols2, mod.ols3, mod.ols4, mod.ols5, mod.ols6, mod.ols7, type ="text", summary= TRUE)
stargazer(mod.ols1, mod.ols2, mod.ols3, mod.ols4, mod.ols5, mod.ols6, mod.ols7, type ="html", out = "statistik.html")
##### Part 5 ###############################################################################################
### Testing Model##
n=nrow(insurance)
set.seed(123)
index = sample(1:n, size = round(n*0.75), replace=FALSE)
insurance.train = insurance[index,]
insurance.test = insurance[-index,]
insurance.expenses=insurance.train
insurance.expenses$logexpenses=NULL
insurance.logexpenses=insurance.train
insurance.logexpenses$expenses=NULL
model1 = lm(expenses~.,data=insurance.expenses)
summary(model1)
vif(model1)
model1_step=step(model1)
summary(model1_step)
vif(model1_step)
model2 = lm(logexpenses~.,data=insurance.logexpenses)
summary(model2)
vif(model2)
model2_step=step(model2)
summary(model2_step)
vif(model2_step)
# Function fro performance
RMSE=function(actual,predicted){
return(sqrt(mean((predicted-actual)^2)))}
# Predictions
predictions.model1=predict(model1, newdata=insurance.test)
predictions.model1_step=predict(model1_step, newdata=insurance.test)
predictions.model2=exp(predict(model2, newdata=insurance.test))
predictions.model2_step=exp(predict(model2_step, newdata=insurance.test))
plot(predictions.model1,insurance.test$expenses)
plot(predictions.model1_step,insurance.test$expenses)
# Results RMSE fro different models
result <- data.frame(Model1=RMSE(predictions.model1,insurance.test$expenses),
Model1_step=RMSE(predictions.model1_step,insurance.test$expenses),
Model2=RMSE(predictions.model2,insurance.test$expenses),
Model2_step=RMSE(predictions.model2_step,insurance.test$expenses))
result
##### Part 6 ###############################################################################################
### Optimization model by adding organic variable cluster k-means
#Trainset clustering expenses: age, bmi and expenses
df.train <- insurance.train[,c(1,3, 7)]
df.scale.train <- as.data.frame(scale(df.train))
head(df.scale.train)
set.seed(123)
expenses_K2.train <- kmeans(df.scale.train, centers = 3, nstart = 25)
fviz_cluster(expenses_K2.train, data = df.scale.train)
insurance.train$cluster <- as.factor(expenses_K2.train$cluster)
#Testset clustering: age, bmi and expenses
df.test <- insurance.test[,c(1,3, 7)]
df.scale.test <- as.data.frame(scale(df.test))
head(df.scale.test)
set.seed(123)
expenses_K2.test <- kmeans(df.scale.test, centers = 3, nstart = 25)
fviz_cluster(expenses_K2.test, data = df.scale.test)
insurance.test$cluster <- as.factor(expenses_K2.test$cluster)
##### Part 7 ###############################################################################################
### Testing Model with cluster##
insurance.expenses=insurance.train
insurance.expenses$logexpenses=NULL
insurance.logexpenses=insurance.train
insurance.logexpenses$expenses=NULL
## Massive better r-squared with 3 groups from 0.75 to 0.89 non log. vif is okay
model1.cl = lm(expenses~.,data=insurance.expenses)
summary(model1.cl)
vif(model1.cl)
model1_step.cl=step(model1.cl)
summary(model1_step.cl)
vif(model1_step.cl)
model2.cl = lm(logexpenses~.,data=insurance.logexpenses)
summary(model2.cl)
vif(model2.cl)
model2_step.cl=step(model2.cl)
summary(model2_step.cl)
vif(model2_step.cl)
predictions.model1.cl=predict(model1.cl, newdata=insurance.test)
predictions.model1_step.cl=predict(model1_step.cl, newdata=insurance.test)
predictions.model2.cl=exp(predict(model2.cl, newdata=insurance.test))
predictions.model2_step.cl=exp(predict(model2_step.cl, newdata=insurance.test))
plot(predictions.model1.cl,insurance.test$expenses)
plot(predictions.model1_step.cl,insurance.test$expenses)
# Results RMSE for different models
result.cluster <- data.frame(Model1.cl=RMSE(predictions.model1.cl,insurance.test$expenses),
Model1_step.cl=RMSE(predictions.model1_step.cl,insurance.test$expenses),
Model2.cl=RMSE(predictions.model2.cl,insurance.test$expenses),
Model2_step.cl=RMSE(predictions.model2_step.cl,insurance.test$expenses))
result.cluster
##### Part 8 ###############################################################################################
# Combining & compaing
stargazer(result, result.cluster, type ="text", summary= FALSE)
stargazer(stat.summary, type="text", summary=F)
# Result linear modell with clustering k-means 3 R2 from 0.75 to around 0.9
# RMSE from 5770 to 3815 40% better
plot(predictions.model1, insurance.test$expenses, xlab="predicted",ylab="actual", main="prediction")
abline(a=0, b= 1)
plot(predictions.model1.cl, insurance.test$expenses, xlab="predicted",ylab="actual")
abline(a=0, b= 1)
plot.pred.act <- data.frame(predictions.model1, insurance.test$expenses)
plot.pred.act.cl <- data.frame(predictions.model1.cl, insurance.test$expenses)
ggplot(plot.pred.act, aes(x=predictions.model1, y= insurance.test$expenses )) +
geom_point(color="blue", alpha = 0.5) + geom_smooth(method = "lm") +
theme(legend.position="none",
plot.title = element_text(size=11)
) +
ggtitle("Predictions vs actual") +
xlab("predicted") + ylab("actual")
ggplot(plot.pred.act.cl, aes(x=predictions.model1.cl, y= insurance.test$expenses )) +
geom_point(color= "red", alpha = 0.5) + geom_smooth(method = "lm") +
theme(legend.position="none",
plot.title = element_text(size=11)) +
ggtitle("Predictions vs actual with cluster") +
xlab("predicted") + ylab("actual")
##### Part 9 ###############################################################################################
# linear Model without region
insurance.expenses$region=NULL
insurance.logexpenses$region=NULL
## Massive better r-squared with 3 groups from 0.75 to 0.89 non log. vif is okay
model1.cl.region = lm(expenses~.,data=insurance.expenses)
summary(model1.cl.region)
vif(model1.cl.region)
model1_step.cl.region=step(model1.cl.region)
summary(model1_step.cl.region)
vif(model1_step.cl.region)
model2.cl.region = lm(logexpenses~.,data=insurance.logexpenses)
summary(model2.cl.region)
vif(model2.cl.region)
model2_step.cl.region=step(model2.cl.region)
summary(model2_step.cl.region)
vif(model2_step.cl.region)
predictions.model1.cl.region=predict(model1.cl.region, newdata=insurance.test)
predictions.model1_step.cl.region=predict(model1_step.cl.region, newdata=insurance.test)
predictions.model2.cl.region=exp(predict(model2.cl.region, newdata=insurance.test))
predictions.model2_step.cl.region=exp(predict(model2_step.cl.region, newdata=insurance.test))
plot(predictions.model1.cl.region,insurance.test$expenses)
plot(predictions.model1_step.cl.region,insurance.test$expenses)
# Results RMSE fro different models
result.cluster.region <- data.frame(Model1.cl.region=RMSE(predictions.model1.cl.region,insurance.test$expenses),
Model1_step.cl.region=RMSE(predictions.model1_step.cl.region,insurance.test$expenses),
Model2.cl.region=RMSE(predictions.model2.cl.region,insurance.test$expenses),
Model2_step.cl.region=RMSE(predictions.model2_step.cl.region,insurance.test$expenses))
result.cluster.region
## With sex, smoker, children, bmi and cluster 0.9r2 and even better RMSE then with region linear model
stargazer(result.cluster, result.cluster.region,  type ="text", summary= FALSE)
ggplot(insurance.expenses, aes(x=bmi, y= expenses)) + geom_point(aes(color=factor(cluster))) +
facet_grid(.~smoker) + ggtitle("smokers vs non smokers")
##### For our Project ###############
# Best approach without variable region (drop). Performance is still okay.
# Modell 1 step ist the best multi
n=nrow(insurance)
set.seed(123)
index = sample(1:n, size = round(n*0.75), replace=FALSE)
insurance.train = insurance[index,]
insurance.test = insurance[-index,]
insurance.expenses=insurance.train
insurance.expenses$logexpenses=NULL
insurance.logexpenses=insurance.train
insurance.logexpenses$expenses=NULL
insurance.expenses$region=NULL
insurance.logexpenses$region=NULL
model1 = lm(expenses~.,data=insurance.expenses)
summary(model1)
vif(model1)
model1_step=step(model1)
summary(model1_step)
vif(model1_step)
model2 = lm(logexpenses~.,data=insurance.logexpenses)
summary(model2)
vif(model2)
model2_step=step(model2)
summary(model2_step)
vif(model2_step)
# Function fro performance
RMSE=function(actual,predicted){
return(sqrt(mean((predicted-actual)^2)))}
# Predictions
predictions.model1=predict(model1, newdata=insurance.test)
predictions.model1_step=predict(model1_step, newdata=insurance.test)
predictions.model2=exp(predict(model2, newdata=insurance.test))
predictions.model2_step=exp(predict(model2_step, newdata=insurance.test))
plot(predictions.model1,insurance.test$expenses)
plot(predictions.model1_step,insurance.test$expenses)
# Results RMSE fro different models
result <- data.frame(Model1=RMSE(predictions.model1,insurance.test$expenses),
Model1_step=RMSE(predictions.model1_step,insurance.test$expenses),
Model2=RMSE(predictions.model2,insurance.test$expenses),
Model2_step=RMSE(predictions.model2_step,insurance.test$expenses))
result
model = lm(expenses~.,data=insurance.expenses)
model_step = step(lm(expenses~.,data=insurance.expenses))
saveRDS(model, file="model.rds")
saveRDS(model_step, file="model.step.rds")
shiny::runApp()
runApp()
runApp()
runApp('OHIE.shiny/OHIE.shiny')
runApp('~/GitHub/BAna-DS')
runApp('OHIE.shiny/OHIE.shiny')
runApp('OHIE.shiny/OHIE.shiny')
runApp('~/GitHub/BAna-DS')
library(Metrics)
library(tidyverse)
library(corrplot)
library(Hmisc)
library(dlstats) #package to check download statistics of packages
library(tree)
library(randomForest)
library(e1071)
library(xgboost)
library(MuMIn)
#Data----
setwd("~/GitHub/BAna-DS")
ohie <- readRDS("OHIE_Wrangled.RDS")
#dataframe with all variables that could generally be included in the model
ohieVariables <- ohie %>%
select(!c("person_id", "treatment", "dt_retro_coverage", "numhh_list", "numhh_list",
"ed_charge_total", "zip_msa_list")) #zip_msa_list would generally work as predictor but it
ohieVariables$sex <- as.numeric(ohieVariables$sex=="Female")
#listwise deletion
ohieVariables_nNA <- drop_na(ohieVariables)
#dataframe with variables removed, that showed to decrease model performance
ohieVariables2 <- select(ohieVariables, !c("any_ed_psychiatric_condition_or_substance_abuse", "food_assistance",
"temporary_assistance"))
ohieVariables_nNA2 <- drop_na(ohieVariables2)
#does not result in any more observations i.e. does not make a difference for model performance if
#imputation method is listwise deletion
set.seed(1111)
index_nNA <- sample(x=c(1:length(ohieVariables_nNA$age)), size=trunc(length(ohieVariables_nNA$age)*0.7),
replace = FALSE)
train_nNA <- ohieVariables_nNA[index_nNA,]
test_nNA <- ohieVariables_nNA[-index_nNA,]
set.seed(1111)
m.bt_3d <- xgb.cv(
data = featuresBt_aci,
label = ohieVariables_nNA$charge_total,
nrounds = 100,
nfold = 5,    #one is for testing the rest is used for training
objective = "reg:squarederror",  # for regression models, linear is depreciated
verbose = 0,   # silent,
max_depth = 3)
#Recalculating the currently best performing model to export it
featuresBt_aci <- as.matrix(select(ohieVariables_nNA, !c("any_ed_psychiatric_condition_or_substance_abuse", "food_assistance",
"temporary_assistance", "charge_total")))
set.seed(1111)
m.bt_3d <- xgb.cv(
data = featuresBt_aci,
label = ohieVariables_nNA$charge_total,
nrounds = 100,
nfold = 5,    #one is for testing the rest is used for training
objective = "reg:squarederror",  # for regression models, linear is depreciated
verbose = 0,   # silent,
max_depth = 3)
saveRDS(m.bt_3d, file"boostingModel")
saveRDS(m.bt_3d, file"~/GitHub/BAna-DS/boostingModel")
?saveRDS
saveRDS(m.bt_3d, file"~/GitHub/BAna-DS")
saveRDS(m.bt_3d, file="~/GitHub/BAna-DS/ohieBoostingModel")
saveRDS(m.bt_3d, file="~/GitHub/BAna-DS/ohieBoostingModel.RDS")
=======
lm(y~x1+x2+x3)
#Aufgabe 2
lm(y~x1+x2)
#Aufgabe 2
summary(lm(y~x1+x2))
x_1 <- 3
x_2 <- 0 + 0.5
lm(y~x_1+x_2)
x_1 <- x1*0 + 3
x_2 <- x2*0 + 0.5
lm(y~x_1+x_2)
rm(x_1)
rm(x_2)
#HANDS ON
y <- c(10, 8, 2, 3, 7)
x1 <- c(1, 4, 2, 2, 0)
x2 <- c(0, 2, 3, 0, 1)
x3 <- c(1, 6, 5, 2, 1)
#Aufgabe 1
lm(y~x1+x2+x3)
#Aufgabe 2
lm(y~x1+x2)
#Aufgabe 2
reg <- lm(y~x1+x2)
summary(reg)
anova(reg)
summary(reg$coefficients)
coefficients(reg)
coefficients(reg[x1])
coefficients(reg)[x1]
coefficients(reg)
coefficients(reg)[2]
coefficients(reg)[2,2]
coefficients(reg)[2,1]
coefficients(reg)[1,2]
coefficients(reg)[1]
coefficients(reg)[2]
coefficients(reg)[2]*3+coefficients(reg)[3]*0
coefficients(reg)[2]*3+coefficients(reg)[3]*0.5
abs(coefficients(reg)[2]*3+coefficients(reg)[3]*0) - abs(coefficients(reg)[2]*3+coefficients(reg)[3]*0.5)
abs(coefficients(reg)[2]*3+coefficients(reg)[3]*0 - coefficients(reg)[2]*3+coefficients(reg)[3]*0.5)
install.packages("htmlLinks")
library(XML)
library(rvest)
library(dplyr)
library(tidyverse)
library(plyr)
library(writexl)
library(htmllink)
#filter out links that are consolidated across all exchanges
url <- getHTMLLinks("http://regsho.finra.org/regsho-Index.html")
xxx <- url[grepl("http://regsho.finra.org/CNMS", url)]
for (s in xxx) {
yyy <- read_delim(s, "|", col_names = TRUE) %>%
na.omit()
yyy <- yyy[yyy$Symbol=="GME",]
yyy <- yyy[1:5]
assign(s, yyy)
next
}
dsv.list <- cbind(mget(xxx))
dsv <- join_all(dsv.list, by = "Date", type = "full")
#filter out links that are consolidated across all exchanges
url <- getHTMLLinks("http://regsho.finra.org/regsho-Index.html")
xxx <- url[grepl("http://regsho.finra.org/CNMS", url)]
for (s in xxx) {
yyy <- read_delim(s, "|", col_names = TRUE) %>%
na.omit()
yyy <- yyy[yyy$Symbol=="GME",]
yyy <- yyy[1:5]
assign(s, yyy)
next
}
dsv.list <- cbind(mget(xxx))
#for other months
fin.mo <- url[grepl("http://regsho.finra.org/regsho", url)]
for (t in fin.mo) {
fin <- getHTMLLinks(t)
xxx <- fin[grepl("http://regsho.finra.org/CNMS", fin)]
for (s in xxx) {
yyy <- read_delim(s, "|", col_names = TRUE) %>%
na.omit()
yyy <- yyy[yyy$Symbol=="GME",]
yyy <- yyy[1:5]
assign(s, yyy)
}
assign(t, dsv.mo.list)
xyx <- cbind(xxx)
}
#for other months
fin.mo <- url[grepl("http://regsho.finra.org/regsho", url)]
for (t in fin.mo) {
fin <- getHTMLLinks(t)
xxx <- fin[grepl("http://regsho.finra.org/CNMS", fin)]
for (s in xxx) {
yyy <- read_delim(s, "|", col_names = TRUE) %>%
na.omit()
yyy <- yyy[yyy$Symbol=="GME",]
yyy <- yyy[1:5]
assign(s, yyy)
}
assign(t, fin.mo)
xyx <- cbind(xxx)
}
dsv.mo.list <- cbind(mget(fin.mo))
`http://regsho.finra.org/regsho-May.html`
shiny::runApp('Documents/R/Option Chain')
#group and plot solutions.
df.long <- option_df%>%
group_by(date, type) %>%
summarise(sumOI = sum(OI, na.rm = TRUE)) %>%
ungroup()
#loop for current option chain.
option <- getOptionChain("GME", NULL)
#add identifiers type and date.
option_df  <- purrr::map_dfr(option, function(d) {
dplyr::bind_rows(d, .id = "type")}, .id = "date")%>%
mutate_at(c("type"), as.factor) %>%
mutate_at(c("date"), anytime::anydate)
runApp('Documents/R/Option Chain')
library(quantmod)
library(dplyr)
shiny::runApp('Documents/R/FINRA Daily Short Sale Volume')
library(tidyverse)
library(neuralnet)
library(caret)
data <- readRDS("OHIE_Final_Selection.RDS")
# imputation
data_nNA1 <- drop_na(data)
summary(data_nNA1)
str(data_nNA1)
data_nNA1$sex <- as.numeric(data_nNA1$sex)
data_nNA1 <- mutate_if(data_nNA1, is.logical, as.numeric)
#Min Max scaling
preproc1 <- preProcess(data_nNA1[,c(1:12,14:15)], method = c("range"))
data_standardized <- predict(preproc1, data_nNA1[,c(1:12,14:15)])
data_standardized <- bind_cols(data_standardized, data_nNA1[,13])
str(data_standardized)
#Test and Train Data
n <- nrow(data_standardized)
set.seed(69)
library(tidyverse)
library(haven)
library(dplyr)
oregonhie_descritpive_vars <- read_dta("oregonhie_descriptive_vars.dta")
oregonhie_ed_vars <- read_dta(file = "oregonhie_ed_vars.dta")
oregonhie_inperson_vars <- read_dta(file = "oregonhie_inperson_vars.dta")
oregonhie_patterns_vars <- read_dta(file = "oregonhie_patterns_vars.dta")
oregonhie_stateprograms_vars <- read_dta(file = "oregonhie_stateprograms_vars.dta")
setwd("~/Documents/Business Analytics/Capstone Project/OHIE_Public_Use_Files/OHIE_Data")
oregonhie_descritpive_vars <- read_dta("oregonhie_descriptive_vars.dta")
oregonhie_ed_vars <- read_dta(file = "oregonhie_ed_vars.dta")
oregonhie_inperson_vars <- read_dta(file = "oregonhie_inperson_vars.dta")
oregonhie_patterns_vars <- read_dta(file = "oregonhie_patterns_vars.dta")
oregonhie_stateprograms_vars <- read_dta(file = "oregonhie_stateprograms_vars.dta")
oregonhie_survey0m_vars <- read_dta(file = "oregonhie_survey0m_vars.dta")
oregonhie_survey12m_vars <- read_dta(file = "oregonhie_survey12m_vars.dta")
oregonhie_survey6m_vars <- read_dta(file = "oregonhie_survey6m_vars.dta")
OHIE_data_for_analysis <- read_dta("~/Documents/Business Analytics/Data/OHIE_data_for_analysis.dta")
#filter all datasets out that we don't want
OHIE_data_filter <- OHIE_data_for_analysis[-unique(c(which(colnames(OHIE_data_for_analysis)%in%colnames(oregonhie_survey12m_vars)),
which(colnames(OHIE_data_for_analysis)%in%colnames(oregonhie_survey6m_vars)),
which(colnames(OHIE_data_for_analysis)%in%colnames(oregonhie_survey0m_vars))))]
OHIE_data_filter<- bind_cols(OHIE_data_for_analysis[1], OHIE_data_filter)
### Normalize data
ggpairs(data_nNA1, title = "Scatterplot Matrix of the Features of the OHIE Data Set")
library(GGally)
### Normalize data
ggpairs(data_nNA1, title = "Scatterplot Matrix of the Features of the OHIE Data Set")
data <- readRDS("OHIE_Final_Selection.RDS")
setwd("~/Documents/R/BAna-DS")
data <- readRDS("OHIE_Final_Selection.RDS")
# imputation
data_nNA1 <- drop_na(data)
### Normalize data
ggpairs(data_nNA1, title = "Scatterplot Matrix of the Features of the OHIE Data Set")
data_nNA1$sex <- as.numeric(data_nNA1$sex)
data_nNA1 <- mutate_if(data_nNA1, is.logical, as.numeric)
#Min Max scaling
preproc1 <- preProcess(data_nNA1[,c(1:12,14:15)], method = c("range"))
data_standardized <- predict(preproc1, data_nNA1[,c(1:12,14:15)])
data_standardized <- bind_cols(data_standardized, data_nNA1[,13])
str(data_standardized)
#Test and Train Data
n <- nrow(data_standardized)
set.seed(69)
index <- sample(1:n, size = round(0.7*n), replace=FALSE)
train <- data_standardized[index,]
test <-data_standardized[-index,]
### formula for NN
feats <- names(data_standardized[,1:14])
# Concatenate strings
f <- paste(feats,collapse=' + ')
f <- paste("charge_total ~",f)
# Convert to formula
f <- as.formula(f)
#First NN
nn1 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex + any_ed_visits +
any_ed_chronic_condition + any_ed_injury + any_ed_skin_condition +
any_ed_abdominal_pain + any_ed_back_pain + any_ed_heart_or_chest_pain +
any_ed_headache + any_ed_depression + charge_food_assistance +
charge_temporary_assistance, data = train)
#First NN
nn1 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex + any_ed_visits +
any_ed_chronic_condition + any_ed_injury + any_ed_skin_condition +
any_ed_abdominal_pain + any_ed_back_pain + any_ed_heart_or_chest_pain +
any_ed_headache + any_ed_depression + charge_food_assistance +
charge_temporary_assistance, data = train)
data <- readRDS("OHIE_Final_Selection.RDS")
# imputation
data_nNA1 <- drop_na(data)
summary(data_nNA1)
str(data_nNA1)
data_nNA1$sex <- as.numeric(data_nNA1$sex)
data_nNA1 <- mutate_if(data_nNA1, is.logical, as.numeric)
#Min Max scaling
preproc1 <- preProcess(data_nNA1[,c(1:12,14:15)], method = c("range"))
data_standardized <- predict(preproc1, data_nNA1[,c(1:12,14:15)])
data_standardized <- bind_cols(data_standardized, data_nNA1[,13])
str(data_standardized)
#Test and Train Data
n <- nrow(data_standardized)
set.seed(69)
index <- sample(1:n, size = round(0.7*n), replace=FALSE)
train <- data_standardized[index,]
test <-data_standardized[-index,]
#First NN
nn1 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex + any_ed_visits +
any_ed_chronic_condition + any_ed_injury + any_ed_skin_condition +
any_ed_abdominal_pain + any_ed_back_pain + any_ed_heart_or_chest_pain +
any_ed_headache + any_ed_depression + charge_food_assistance +
charge_temporary_assistance, data = train)
plot(nn1, rep = 'best')
#First NN
nn1 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex, act.fct = "Relu")
#First NN
nn1 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex, data = train, act.fct = "Relu")
#First NN
#activation function
Relu <- function(x) max(0,x)
nn1 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex, data = train, act.fct = Relu)
#First NN
#activation function
max(0,1)
#First NN
#activation function
max(0,10)
softplus <-
nn1 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex, data = train, act.fct = softplus)
softplus <- function(x) log(1+exp(x))
nn1 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex, data = train, act.fct = softplus)
nn1 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex, data = train, linear.output = FALSE, act.fct = softplus)
nn1 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex, data = train, linear.output = FALSE, act.fct = softplus)
nn1 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex, data = train, linear.output = FALSE, hidden = c(3, 2), act.fct = softplus)
nn1 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex, data = train,
hidden = c(3, 2), act.fct = softplus)
#First NN
#activation function
#would love to use ReLU but non-differentiability not supported by neuralnet package.
#softplus as smooth approximation of ReLU
softplus(20)
#First NN
#activation function
#would love to use ReLU but non-differentiability not supported by neuralnet package.
#softplus as smooth approximation of ReLU
softplus(1)
#First NN
#activation function
#would love to use ReLU but non-differentiability not supported by neuralnet package.
#softplus as smooth approximation of ReLU
softplus(4)
#First NN
#activation function
#would love to use ReLU but non-differentiability not supported by neuralnet package.
#softplus as smooth approximation of ReLU
softplus(5)
#First NN
#activation function
#would love to use ReLU but non-differentiability not supported by neuralnet package.
#softplus as smooth approximation of ReLU
softplus(6)
#First NN
#activation function
#would love to use ReLU but non-differentiability not supported by neuralnet package.
#softplus as smooth approximation of ReLU
softplus(10)
#First NN
#activation function
#would love to use ReLU but non-differentiability not supported by neuralnet package.
#softplus as smooth approximation of ReLU
softplus(11)
#First NN
#activation function
#would love to use ReLU but non-differentiability not supported by neuralnet package.
#softplus as smooth approximation of ReLU
softplus(12)
#First NN
#activation function
#would love to use ReLU but non-differentiability not supported by neuralnet package.
#softplus as smooth approximation of ReLU
softplus(13)
#First NN
#activation function
#would love to use ReLU but non-differentiability not supported by neuralnet package.
#softplus as smooth approximation of ReLU
softplus(12.6)
nn1 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex, data = train, threshold = 0.2
, act.fct = softplus, lifesign = "minimal")
nn1 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex, data = train, stepmax = 1e+07, threshold = 0.2
, act.fct = softplus, lifesign = "full")
nn1 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex, data = train, stepmax = 1e+07,
, act.fct = softplus, lifesign = "full")
#Min Max scaling
preproc1 <- preProcess(data_nNA1, method = c("range"))
data_standardized <- predict(preproc1, data_nNA1
data_standardized <- bind_cols(data_standardized, data_nNA1[,13])
str(data_standardized)
#Test and Train Data
n <- nrow(data_standardized)
set.seed(69)
index <- sample(1:n, size = round(0.7*n), replace=FALSE)
train <- data_standardized[index,]
test <-data_standardized[-index,]
#First NN
#activation function
#would love to use ReLU but non-differentiability not supported by neuralnet package.
#softplus as smooth approximation of ReLU
softplus <- function(x) log(1+exp(x))
nn1 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex, data = train, stepmax = 1e+07,
act.fct = softplus, lifesign = "full")
+ any_ed_visits +
any_ed_chronic_condition + any_ed_injury + any_ed_skin_condition +
any_ed_abdominal_pain + any_ed_back_pain + any_ed_heart_or_chest_pain +
any_ed_headache + any_ed_depression + charge_food_assistance +
charge_temporary_assistance, data = train)
data_standardized <- predict(preproc1, data_nNA1)
View(data_standardized)
#Test and Train Data
n <- nrow(data_standardized)
set.seed(69)
index <- sample(1:n, size = round(0.7*n), replace=FALSE)
train <- data_standardized[index,]
test <-data_standardized[-index,]
nn1 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex, data = train, stepmax = 1e+07,
act.fct = softplus, lifesign = "full")
plot(nn1, rep = 'best')
nn1 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex + any_ed_visits +
any_ed_chronic_condition + any_ed_injury + any_ed_skin_condition +
any_ed_abdominal_pain + any_ed_back_pain + any_ed_heart_or_chest_pain +
any_ed_headache + any_ed_depression + charge_food_assistance +
charge_temporary_assistance, data = train, stepmax = 1e+07, threshold = 0.02,
act.fct = softplus, lifesign = "full"))
nn1 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex + any_ed_visits +
any_ed_chronic_condition + any_ed_injury + any_ed_skin_condition +
any_ed_abdominal_pain + any_ed_back_pain + any_ed_heart_or_chest_pain +
any_ed_headache + any_ed_depression + charge_food_assistance +
charge_temporary_assistance, data = train, stepmax = 1e+07, threshold = 0.02,
act.fct = softplus, lifesign = "full")
plot(nn1, rep = 'best')
View(nn1)
View(nn1)
View(data_nNA1)
View(data_standardized)
NN1_Train_SSE <- sum((nn1$net.result - train[, 13])^2)/2
paste("SSE: ", round(train, 4))
paste("SSE: ", round(NN1_Train_SSE, 4))
#SSE of test
Test_nn1_output <- predict(nn1, test[, c(1:12,14,15)])$net.result
NN1_Test_SSE <- sum((Test_nn1_Output - test[, 13])^2)/2
#SSE of test
Test_nn1_output <- predict(nn1, test[, c(1:12,14,15)])
View(Test_nn1_output)
#SSE of test
Test_nn1_output <- predict(nn1, test[13])
NN1_Test_SSE <- sum((Test_nn1_Output - test[, 13])^2)/2
NN1_Test_SSE
NN1_Test_SSE
NN1_Test_SSE <- sum((Test_nn1_Output - test[, 13])^2)/2
NN1_Test_SSE <- sum((Test_nn1_Output - test[13])^2)/2
#SSE of test
Test_nn1_output <- predict(nn1, test[, 13])
test[, 13]
#SSE of test
Test_nn1_output <- predict(nn1, test[, 13])
test[13]
#SSE of test
Test_nn1_output <- predict(nn1, test[13])
NN1_Test_SSE <- sum((Test_nn1_output - test[13])^2)/2
#SSE of test
Test_nn1_output <- predict(nn1, test[, 1:12,14,15])
View(test)
#SSE of test
Test_nn1_output <- predict(nn1, test[, c(1:12, 14,15)])
NN1_Test_SSE <- sum((Test_nn1_output - test[13])^2)/2
NN1_Test_SSE
paste("SSE: ", round(NN1_Test_SSE, 4))
#
nn2 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex + any_ed_visits +
any_ed_chronic_condition + any_ed_injury + any_ed_skin_condition +
any_ed_abdominal_pain + any_ed_back_pain + any_ed_heart_or_chest_pain +
any_ed_headache + any_ed_depression + charge_food_assistance +
charge_temporary_assistance, data = train, stepmax = 1e+07, threshold = 0.02,
act.fct = logistic, lifesign = "full")
#
nn2 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex + any_ed_visits +
any_ed_chronic_condition + any_ed_injury + any_ed_skin_condition +
any_ed_abdominal_pain + any_ed_back_pain + any_ed_heart_or_chest_pain +
any_ed_headache + any_ed_depression + charge_food_assistance +
charge_temporary_assistance, data = train, stepmax = 1e+07, threshold = 0.02,
act.fct = "logistic", lifesign = "full")
#SSE of train
nn2_Train_SSE <- sum((nn2$net.result - train[, 13])^2)/2
paste("SSE train: ", round(nn2_Train_SSE, 4))
#SSE of test
Test_nn2_output <- predict(nn2, test[, c(1:12, 14,15)])
nn2_Test_SSE <- sum((Test_nn2_output - test[13])^2)/2
paste("SSE test: ", round(nn2_Test_SSE, 4))
#1 Hidden layer, 1 Neuron ANN, logistic activation, threshold 0.01
nn2 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex + any_ed_visits +
any_ed_chronic_condition + any_ed_injury + any_ed_skin_condition +
any_ed_abdominal_pain + any_ed_back_pain + any_ed_heart_or_chest_pain +
any_ed_headache + any_ed_depression + charge_food_assistance +
charge_temporary_assistance, data = train, stepmax = 1e+07, threshold = 0.01,
act.fct = "logistic", lifesign = "full")
#SSE of train
nn2_Train_SSE <- sum((nn2$net.result - train[, 13])^2)/2
#SSE of test
Test_nn2_output <- predict(nn2, test[, c(1:12, 14,15)])
logistic
####### nn 3 #########
#9 Hidden layer (aggregation of 2/3 of input layer.), 1 Neuron ANN, logistic activation, threshold 0.02
#computation time ≈30sec
nn3 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex + any_ed_visits +
any_ed_chronic_condition + any_ed_injury + any_ed_skin_condition +
any_ed_abdominal_pain + any_ed_back_pain + any_ed_heart_or_chest_pain +
any_ed_headache + any_ed_depression + charge_food_assistance +
charge_temporary_assistance, data = train, hidden = c(9,1), stepmax = 1e+07, threshold = 0.02,
act.fct = "logistic", lifesign = "full")
plot(nn3, rep = 'best')
#SSE of train
nn3_Train_SSE <- sum((nn3$net.result - train[, 13])^3)/3
#SSE of test
Test_nn3_output <- predict(nn3, test[, c(1:13, 14,15)])
nn3_Test_SSE <- sum((Test_nn3_output - test[13])^3)/3
#SSE of train
nn3_Train_SSE <- sum((nn3$net.result - train[, 13])^2)/2
#SSE of test
Test_nn3_output <- predict(nn3, test[, c(1:13, 14,15)])
nn3_Test_SSE <- sum((Test_nn3_output - test[13])^2)/2
######## nn 4 #########
nn4 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex + any_ed_visits +
any_ed_chronic_condition + any_ed_injury + any_ed_skin_condition +
any_ed_abdominal_pain + any_ed_back_pain + any_ed_heart_or_chest_pain +
any_ed_headache + any_ed_depression + charge_food_assistance +
charge_temporary_assistance, data = train, hidden = c(9,1), stepmax = 1e+07, threshold = 0.02,
act.fct = "softplus", lifesign = "full")
######## nn 4 #########
nn4 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex + any_ed_visits +
any_ed_chronic_condition + any_ed_injury + any_ed_skin_condition +
any_ed_abdominal_pain + any_ed_back_pain + any_ed_heart_or_chest_pain +
any_ed_headache + any_ed_depression + charge_food_assistance +
charge_temporary_assistance, data = train, hidden = c(9,1), stepmax = 1e+07, threshold = 0.02,
act.fct = softplus, lifesign = "full")
plot(nn4, rep = 'best')
plot(nn3, rep = 'best')
plot(nn4, rep = 'best')
######## nn 4 #########
nn4 <- neuralnet(charge_total ~ preperiod_any_visits + age + sex + any_ed_visits +
any_ed_chronic_condition + any_ed_injury + any_ed_skin_condition +
any_ed_abdominal_pain + any_ed_back_pain + any_ed_heart_or_chest_pain +
any_ed_headache + any_ed_depression + charge_food_assistance +
charge_temporary_assistance, data = train, hidden = c(9,1), stepmax = 1e+07, threshold = 0.02,
act.fct = softplus, lifesign = "full")
#SSE of train
nn4_Train_SSE <- sum((nn4$net.result - train[, 13])^2)/2
paste("SSE train: ", round(nn4_Train_SSE, 4))
#SSE of test
Test_nn4_output <- predict(nn4, test[, c(1:13, 14,15)])
nn4_Test_SSE <- sum((Test_nn4_output - test[13])^2)/2
Regression_nn_Errors <- tibble(Network = rep(c("nn1", "nn2", "nn3", "nn4"), each = 2),
DataSet = rep(c("Train", "Test"), time = 4),
SSE = c(nn1_Train_SSE, nn1_Test_SSE,
nn2_Train_SSE, nn2_Test_SSE,
nn3_Train_SSE, nn3_Test_SSE,
nn4_Train_SSE, nn4_Test_SSE))
#SSE of train
nn1_Train_SSE <- sum((nn1$net.result - train[, 13])^2)/2
nn1_Test_SSE <- sum((Test_nn1_output - test[13])^2)/2
rm(NN1_Test_SSE)
rm(NN1_Train_SSE)
Regression_nn_Errors %>%
ggplot(aes(Network, SSE, fill = DataSet)) +
geom_col(position = "dodge") +
ggtitle("Regression ANN's SSE")
Regression_nn_Errors <- tibble(Network = rep(c("nn1", "nn2", "nn3", "nn4"), each = 2),
DataSet = rep(c("Train", "Test"), time = 4),
SSE = c(nn1_Train_SSE, nn1_Test_SSE,
nn2_Train_SSE, nn2_Test_SSE,
nn3_Train_SSE, nn3_Test_SSE,
nn4_Train_SSE, nn4_Test_SSE))
Regression_nn_Errors %>%
ggplot(aes(Network, SSE, fill = DataSet)) +
geom_col(position = "dodge") +
ggtitle("Regression ANN's SSE")
#the more complex our NN , the worse it actually gets.
#NN is having a really hard time working through the sparse underlying dataset.
save(nn2, file="nn2.Rdata")
save(nn4, file="nn4.Rdata")
View(Regression_nn_Errors)
save.image("~/Documents/Business Analytics/Capstone Project/NN.RData")
View(preproc1)
plot(nn4, rep = 'best')
>>>>>>> Stashed changes
