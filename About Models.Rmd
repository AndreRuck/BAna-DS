---
title: "About Models"
author: "Zoran Dobrosavljevic, Thibaud Mottier, Ryu Pape, André Ruckdäschel"
date: "5/17/2021"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(neuralnet)
library(caret)
```


## Data wrangling/pre-processing
Only applies to OHIE dataset
```{r load file and models for reporting, include=FALSE}
load("NN.Rdata")
```
## Content Library
- Insurance.csv
  - Multiple Linear Regression
- OHIE
  - Multiple Linear Regression
  - Simple Tree Regression
  - Random Forest Regression
  - Gradient boosting Regression(best performing)
  - Artificial Neural Networks Regression

# insurance.csv

## Underlying Algorithm behind predictions
Mutliple Linear Regression
```{r cars, echo = TRUE}

```
# OHIE_Final_Selection.RDS

## Multiple Linear Regression

```{r pressure}

```

## Simple Tree Regression
```{r }

```
## Random Forest Regression

## Gradient boosting Regression

## Artificial Neural Network Regression
A Neural Network with 1 Hidden Layer and 1 Neuron, using a softplus activation function.
```{r nn1 Neural Network}
plot(nn2, rep = "best")
```

## Artificial Neural Network Regression
A Neural Network with 9 Hidden Layers and 1 Neuron, using a softplus activation function
```{r nn4 Neural Network}
plot(nn4, rep = "best")
```
  
  
## Artificial Neural Network Regression
nn1: 1 Hidden Layer, 1 Neuron, logistic activation function  
nn2: 1 Hidden Layer, 1 Neuron, softplus activation function  
nn3: 9 Hidden Layer, 1 Neuron, logistic activation function  
nn4: 9 Hidden Layer, 1 Neuron, softplus activation function  
<br />
For our Neural Network the graphic on the next slide shows that the more complex our NN, the worse it .  
NN is having a really hard time working through the sparse underlying dataset. 


## Artificial Neural Network Regression
```{r comparing different Models}
Regression_nn_Errors %>% 
  ggplot(aes(Network, SSE, fill = DataSet)) + 
  geom_col(position = "dodge") + 
  ggtitle("Regression ANN's SSE")
```
