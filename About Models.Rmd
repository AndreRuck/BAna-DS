---
title: "About Models"
author: "Zoran Dobrosavljevic, Thibaud Mottier, Ryu Pape, André Ruckdäschel"
date: "5/17/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(neuralnet)
library(caret)
library(tree)
```


## Data wrangling/pre-processing
Only applies to OHIE dataset
```{r load file and models for reporting, include=FALSE}
#setwd("~/GitHub/BAna-DS")
load("NN.Rdata")

m.bt_aci <- readRDS("mbt_aci.RDS")
depthList_evaluation <- readRDS("depthList_evaluation.RDS")
#load("dredlm.RDS")
ohie <- readRDS("OHIE_Wrangled.RDS")

#dataframe with all variables that could generally be included in the model
ohieVariables <- ohie %>% 
  select(!c("person_id", "treatment", "dt_retro_coverage", "numhh_list", "numhh_list",
                                "ed_charge_total", "zip_msa_list")) #zip_msa_list would generally work as predictor but it
#contains only one possible value, which renders it useless for actual predictions
ohieVariables$sex <- as.numeric(ohieVariables$sex=="Female")

#Creating train and test data with different imputation methods

#no imputation
set.seed(1111)
index <- sample(x=c(1:length(ohie$person_id)), size=trunc(length(ohie$person_id)*0.7), replace = FALSE)
train <- ohieVariables[index,]
test <- ohieVariables[-index,]

#listwise deletion
ohieVariables_nNA <- drop_na(ohieVariables)
#dataframe with variables removed, that showed to decrease model performance
ohieVariables2 <- select(ohieVariables, !c("any_ed_psychiatric_condition_or_substance_abuse", "food_assistance",
                                           "temporary_assistance"))
ohieVariables_nNA2 <- drop_na(ohieVariables2)
#does not result in any more observations i.e. does not make a difference for model performance if 
#imputation method is listwise deletion
set.seed(1111)
index_nNA <- sample(x=c(1:length(ohieVariables_nNA$age)), size=trunc(length(ohieVariables_nNA$age)*0.7),
                    replace = FALSE)
train_nNA <- ohieVariables_nNA[index_nNA,]
test_nNA <- ohieVariables_nNA[-index_nNA,]
```
## Content Library
- Insurance.csv
  - Multiple Linear Regression
- OHIE
  - Multiple Linear Regression
  - Support Vector Regression
  - Simple Tree Regression
  - Random Forest Regression
  - Gradient boosting Regression(best performing)
  - Artificial Neural Networks Regression

# insurance.csv

## Underlying Algorithm behind predictions
Mutliple Linear Regression
```{r cars, echo = TRUE}

```
# OHIE_Final_Selection.RDS

## Multiple Linear Regression
Due to their low computational intensity, linear regressions were used as a tool for further feature selection as well as a possible algorithm for our final data product.
Within the undertaking of feature selection, some seemingly mutually-exclusive variables were chosen through manual comparisons of different models. Subsequently the akaike information criterions for every possible variable combination were computed.
The linear regression model with the highest AIC:
```{r pressure}
ggplot(train_nNA, aes(y=charge_total)) +
  #geom_point() +
  geom_smooth(method='lm', aes(x=age))
```
## Support Vector Regression
To incorporate possible non-linear relationships better, a support vectror regression was used to predict expenses. However, primarily due to the problem of overfitting, the initial model performed quite poorly. Whilst optimzuations, such as the stricter feature selection on the basis of AICs resulted in significant improvements to model performance, the overall performance of support vector regressions continued to lag behind both linear and tree based approaches. Support vector based modelling was thus not further explored and optimized. Instead, our team focused on tree based approaches.

## Simple Tree Regression
Single trees can generally be expected to underperform tree based ensembler algorithms such as random forests or gradient boosting. We nevertheless elected to use a simple tree regression as a starting point for our analysis through tree based models, due to its low computation cost and ease of interpretability.
This was the resulting regression tree:
```{r, include=FALSE}
m.tr <- tree(charge_total ~ ., train_nNA)
p.tr <- predict(m.tr, newdata = test_nNA)
```
```{r }
plot(m.tr)
text(m.tr, pretty = 0)
```
Interestingly, this initial regression tree elected solely criteria from the users medical history as decision factors.

## Random Forest Regression
The single tree performed worse than the optimized linear regression, which appeared to be largely due to problems of overfitting. Through the use of random forests, this performance could be enhanced significantly. The best performing random forest, namely the ranodom forest using the AIC optimized features within the linear gregression model, outperformed the linear regression with ACI optimized features and interaction terms with the variable sex.
```{r}

```

##Gradient Boosting Regressing utilizing decision trees
In addition to the single tree and random forests, we implemented gradient boosting, as a third tree based method. Within this method, new trees are constructed using the information of already constructed trees, which differentiates it from random forests where additional tress are constructed independently of the already constructed trees. The problem of overfitting, was even more apparent under this algorithm.
The following graph of the in sample and out of sample error rates of the boosting model with the AIC optimized features illustrates the influence of overfitting especially clearly:
```{r}
ggplot(m.bt_aci$evaluation_log, aes(x=iter)) +
  geom_line(aes(y=train_rmse_mean), color = "red", show.legend = TRUE) +
  geom_line(aes(y=test_rmse_mean), color = "blue", show.legend = TRUE) + 
  annotate(geom = "text", x = 1000, y = 4800, label = "In sample error", color = "red") +
  annotate(geom = "text", x = 1000, y = 18000, label = "Out of sample error", color = "blue") +
  labs(title = "Comparing the evolution of in and out of sample error rates as the number of iterations increases")
```
Subsequent of the tweaking of factors such as the number of iterations and the tree depth to avoid overfitting, the gradient boosting model turned out to be the best performing model overall. It it thus the model, which our service utilizes.
```{r}
ggplot(depthList_evaluation, aes(x=treeDepth, y=rmse.test)) +
  geom_col() +
  labs(title = "Smallest out of sample error per tree depth")
```


## Artificial Neural Network Regression
because NN haven't been covered in class we'll quickly show what 
A Neural Network with 1 Hidden Layer and 1 Neuron, using a softplus activation function.
```{r nn1 Neural Network}
plot(nn2, rep = "best")
```

A Neural Network with 9 Hidden Layers and 1 Neuron, using a softplus activation function
```{r nn4 Neural Network}
plot(nn4, rep = "best")
```
  
nn1: 1 Hidden Layer, 1 Neuron, logistic activation function  
nn2: 1 Hidden Layer, 1 Neuron, softplus activation function  
nn3: 9 Hidden Layer, 1 Neuron, logistic activation function  
nn4: 9 Hidden Layer, 1 Neuron, softplus activation function  
<br />
For our Neural Network the graphic on the next slide shows that the more complex our NN, the worse it .  
NN is having a really hard time working through the sparse underlying dataset. 



```{r comparing different Models}
Regression_nn_Errors %>% 
  ggplot(aes(Network, SSE, fill = DataSet)) + 
  geom_col(position = "dodge") + 
  ggtitle("Regression ANN's SSE")
```
